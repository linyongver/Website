

<meta name="description" content="Yong Lin's homepage">
<link rel="stylesheet" href="./files/jemdoc.css" type="text/css">
<title>Yong Lin's Homepage</title>


<body>

    <style>
        /*********************************
         The list of publication items
         *********************************/
    /* The list of items */
    .biblist { }
    
    /* The item */
    .biblist li { }
    
    /* You can define custom styles for plstyle field here. */
    
    
    /*************************************
     The box that contain BibTeX code
     *************************************/
    div.noshow { display: none; }
    div.bibtex {
      margin-right: 0%;
      margin-top: 1.2em;
      margin-bottom: 1em;
      border: 1px solid silver;
      padding: 0em 1em;
      background: #ffffee;
    }
    div.bibtex pre { font-size: 80%; overflow: auto;  width: 100%; padding: 1em 0em;}</style>
    <script type="text/javascript">
        <!--
        // Toggle Display of BibTeX
        function toggleBibtex(articleid) {
            var bib = document.getElementById('bib_'+articleid);
            if (bib) {
                if(bib.className.indexOf('bibtex') != -1) {
                    bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex';
                }
            } else {
                return;
            }
        }
    -->
        </script>

<div id="layout-content" style="margin-top:25px">

<table>
    <tbody>
        <tr>
            <td width="670">
                <div id="toptitle">                 
                    <h1><b>Yong Lin</b>&nbsp;
                        </div>
                
<!--                 <h3>Ph.D. Student</h3>  
                <p>
                    <a href="https://hkust.edu.hk/" target=&ldquo;blank&rdquo;>Department of Computer Science and Engineering</a> <br>
                    <a href="https://hkust.edu.hk/" target=&ldquo;blank&rdquo;>Hong Kong  University of Science and Technology</a> <br>
                    Hong Kong<br>
                </p> -->

                
                <h3> Postdoctoral Fellow</h3>  
                <p>
                    <a href="https://pli.princeton.edu/" target=&ldquo;blank&rdquo;> Princeton Language and Intellegence, </a> <a href="https://www.princeton.edu/" target=&ldquo;blank&rdquo;> Princeton University</a>  <br>
                    New Jersey<br>
                </p>
                <p>
                    <a href="https://scholar.google.com/citations?user=M4g0ZvMAAAAJ&hl=en"><b>[Google Scholar]</b></a>
                    <a href="https://github.com/linyongver" target="_blank"><b>[Github]</b></a>  <br>
                    <a href="https://x.com/Yong18850571" target="_blank"><b>[Twitter]</b></a>  <br>
                    E-mail: yong.lin [at] princeton [dot] edu <br>
                    
                </p>
            </td>
            <td>
                <img src="./files/self.png" border="0" width="360">
            </td>
        </tr><tr>
    </tr></tbody>
</table>
<h2>Short Bio</h2>
<p>
     I am a postdoctoral fellow at <a href="https://pli.princeton.edu/" target="_blank">Princeton Language and Intelligence</a>(PLI), working with  <a href="https://sites.google.com/view/cjin/home" target="_blank">Chi Jin</a>,  <a href="https://www.cs.princeton.edu/~arora/" target="_blank">Sanjeev Arora</a>, and  <a href="https://www.cs.princeton.edu/~danqic/" target="_blank">Danqi Chen</a>.  I did my PhD study in <a href="https://tongzhang-ml.org/" target="_blank">Tong Zhang</a>'s group.  
</p>

<p>
     My research focuses on machine learning  and the post training of large language models (LLMs). Key areas of interest include:
    <ul>
        <li>
            (1) <b>Formal Math Reasoning</b>. Enable LLMs to reason using verifiable languages, also known as formal languages, such as <a href="https://leanprover-community.github.io/" target=&ldquo;blank&rdquo;>LEAN</a>. Reasoning in formal languages is also referred to as automated theorem proving, the correctness of which can be verified through a compiler. I am currently co-leading the <a href="https://goedel-lm.github.io/" target=&ldquo;blank&rdquo;> Goedel-Prover </a>  project at <a href="https://pli.princeton.edu/" target=&ldquo;blank&rdquo;> Princeton Language and Intellegence, </a> to train LLMs for automated theorem proving. We have just released a SOTA 32B and 8B open-source model <a href="https://blog.goedel-prover.com/" target=&ldquo;blank&rdquo;> here</a>.
        </li>
        <li>
            (2) <b>Post-Training of LLMs</b> to prioritize traits like helpfulness, harmlessness, and honesty. Our paper, <a href="https://arxiv.org/pdf/2311.09677.pdf" target="_blank">R-tuning</a>, reveals that current standard instruction-tuning methods can aggravate hallucinations and further shows strategies to mitigate them. This work won the <a href="https://2024.naacl.org/" target="_blank">Outstanding Paper Award</a> at NAACL 2024. I also explored ways to mitigate the alignment tax of RLHF (e.g., <a href="https://arxiv.org/pdf/2309.06256" target="_blank">HMA</a>) and align LLMs with diverse human preferences (e.g., <a href="https://arxiv.org/pdf/2402.18571" target="_blank">DPA</a>).
        </li>     
    </ul>
    
</p>

<p>
     Before starting my PhD studies, I served as a Senior Machine Learning Engineer at Alibaba from 2017 to 2021, a prominent company in China (which open-sourced <a href="https://qwenlm.github.io/blog/qwen2/" target="_blank">Qwen</a> series models). I experienced firsthand the impressive capabilities of machine learning and developed industrial-level applications. Concurrently, I gained insights into the inherent challenges and instability of deep models in industrial settings. During early years of my PhD study, I also worked on Out-of-Distribution Generalization problems, such as enabling an autonomous driving system trained on city roads to navigate country roads; ensuring AI diagnostic systems trained on data from one hospital can reliably predict patients from another hospital. 
</p>

<p>  I was an awardee of <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2023" target="_blank">Apple AI/ML PhD fellowship</a> (2023) and <a href="https://cerg1.ugc.edu.hk/hkpfs/index.html" target="_blank">Hong Kong PhD fellowship</a> (2020). 
</p>
    
<h3>News</h3>
<li>
    July 2025, we released <a href="https://blog.goedel-prover.com/" target="_blank"> Goedel-Prover-V2 </a> , ranking <font color="Red"><b>1st</b></font>  on <a href="https://trishullab.github.io/PutnamBench/leaderboard.html" target="_blank"> PutnamBench Leaderboard</a> (again), significantly beating the previous SOTA <a href="https://github.com/deepseek-ai/DeepSeek-Prover-V2" target="_blank">Deepseek-Prover-V2-671B</a>.   
</li>
<li>
    Feb 2025, I served as an Area Chair of <a href="https://aclrollingreview.org/" target="_blank"> ACL ARR </a>.  
</li>
<li>
    Feb 2025, we released <a href="https://goedel-lm.github.io/" target="_blank"> Goedel-Prover </a>  for automated theorem proofing, ranking <font color="Red"><b>1st</b></font>  on <a href="https://trishullab.github.io/PutnamBench/leaderboard.html" target="_blank"> PutnamBench Leaderboard</a>.  
</li>
<li>
    Oct 2024, our method <a href="https://arxiv.org/abs/2502.00674" target="_blank">SelfMoA</a> ranked <font color="Red"><b>1st</b></font> on <a href="https://tatsu-lab.github.io/alpaca_eval/" target="_blank"> AlpacaEval 2.0 Leaderboard</a>. 
</li>
<li>
    Aug 2024, I joined <a href="https://pli.princeton.edu/" target="_blank">Princeton Language and Intelligence</a> as a Postdoc Fellow
</li>

<li>
    Jun 2024, our paper <a href="https://arxiv.org/pdf/2311.09677.pdf" target="_blank">R-tuning</a> won the Outstanding Paper Award of NAACL.
</li>
<h2>Selected Papers</h2>
<p> (* denotes equal or core contribution) </p>
<ul>
    <h3>Pre-prints</h3>
    <li>
        <b>Yong Lin</b>*, Shange Tang*, Bohan Lyu*, Ziran Yang*, Jui-Hui Chung*,  Haoyu Zhao*,  Lai Jiang*,  Yihan Geng*,  Jiawei Ge,  Jingruo Sun,  Jiayun Wu ,  Jiri Gesi, David Acuna, Kaiyu Yang, Hongzhou Lin*, Yejin Choi, Danqi Chen, Sanjeev Arora, Chi Jin* <br>
        <a href="https://blog.goedel-prover.com/" target=&ldquo;blank&rdquo;>Goedel-Prover-V2: The Strongest Open-Source Theorem Prover to Date.</a> <br>              
        Blog. <br>
    </li>
    
    <li>
        Wenzhe Li*, <b>Yong Lin</b>*, Mengzhou Xia, Chi Jin <br>
        <a href="https://arxiv.org/abs/2502.00674" target=&ldquo;blank&rdquo;>Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?</a> <br>              
        Pre-prints. <br>
    </li>

    <li>
        Hanning Zhang, Pengcheng Wang, Shizhe Diao,<b>Yong Lin</b>, Rui Pan, Hanze Dong, Dylan Zhang, Pavlo Molchanov, Tong Zhang <br>
        <a href="https://arxiv.org/pdf/2412.11006" target=&ldquo;blank&rdquo;>Entropy-Regularized Process Reward Model.</a> <br>              
        Pre-prints. <br>
    </li>
    

    <li>
        Yifan Hao*, <b>Yong Lin*</b>, Difan Zou, Tong Zhang. <br>
        <a href="https://arxiv.org/pdf/2403.17592.pdf" target=&ldquo;blank&rdquo;>On the Benefits of Over-parameterization for Out-of-Distribution Generalization.</a> <br>              
        Annals of Statistics in submission.<br>
    </li>
    


    
    <h3>Publications</h3>
    <li>
        <b>Yong Lin</b>*, Shange Tang*, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia Li, Mengzhou Xia, Danqi Chen, Sanjeev Arora, Chi Jin<br>
        <a href="https://arxiv.org/abs/2502.07640" target=&ldquo;blank&rdquo;>Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving.</a> <br>              
        COLM 2025. <br>
    </li> 
    <li>
        <b>Yong Lin*</b>, Chen Liu*, Chenlu Ye*, Qing Lian, Yuan Yao, Tong Zhang. <br>
        <a href="https://arxiv.org/abs/2309.02476" target=&ldquo;blank&rdquo;>Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning.</a> <br>              
        JMLR (accepted). <br>
    </li>

   <li>
        Qizhou Wang*, <b>Yong Lin*</b>, Yongqiang Chen*, Ludwig Schmidt, Bo Han, Tong Zhang<br>
        <a href="https://counteranimal.github.io/" target=&ldquo;blank&rdquo;>Do CLIPs Always Generalize Better than ImageNet Models?</a> <br>              
        NeurIPS 2024.<br>
    </li>

    <li>
        <b>Yong Lin*</b>, Hangyu Lin*, Wei Xiong*, Shizhe Diao*,[+8 authors], Han Zhao , Nan Jiang, Heng Ji, Yuan Yao, and Tong Zhang.<br>
        <a href="https://arxiv.org/pdf/2309.06256.pdf" target=&ldquo;blank&rdquo;>Mitigating the Alignment Tax of RLHF.</a> <br>              
        ENMLP 2024. [<a href="https://github.com/avalonstrel/AdaptiveMA" target=&ldquo;blank&rdquo;> code </a>]  <br>
    </li>

    <li>
        <b>Yong Lin*</b>, Skyler Seto*, Maartje ter Hoeve, Katherine Metcalf, Barry-John Theobald, Xuan Wang, Yizhe Zhang, Chen Huang, Tong Zhang <br>
        <a href="https://arxiv.org/pdf/2409.03650" target=&ldquo;blank&rdquo;>On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization.</a> <br>              
        ENMLP 2024 Findings.  <br>
    </li>
    
    <li>
        Haoxiang Wang*, <b>Yong Lin*</b>, Wei Xiong*, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang<br>
        <a href="https://arxiv.org/pdf/2402.18571.pdf" target=&ldquo;blank&rdquo;>Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards.</a> <br>              
        ACL 2024.<br>
    </li>

    <li>
        Hanning Zhang*, Shizhe Diao*, <b>Yong Lin*</b>, Yi R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, Tong Zhang. <br>
        <a href="https://arxiv.org/pdf/2311.09677.pdf" target=&ldquo;blank&rdquo;>R-tuning: Teaching large language models to refuse unknown questions.</a> <br>              
        NAACL 2024 [<font color="Red"><b>Outstanding Paper Award, 6/2434 = 0.25%</b></font>] . <br>
    </li>
    
    <li>
        <b>Yong Lin*</b>, Lu Tan*, Yifan Hao*, Honam Wong, Hanze Dong, Weizhong Zhang, Yujiu Yang, Tong Zhang. <br>
        <a href="https://arxiv.org/pdf/2309.17230.pdf" target=&ldquo;blank&rdquo;>Spurious Feature Diversification Improves Out-of-distribution Generalization.</a> <br>   
        ICLR 2024.<br>
    </li>

    <li>
        Damien Teney, <b>Yong Lin</b>, Seong Joon Oh, Ehsan Abbasnejad. <br>
        <a href="https://arxiv.org/pdf/2209.00613" target=&ldquo;blank&rdquo;>Id and ood performance are sometimes inversely correlated on real-world datasets.</a> <br>   
        NeurIPS 2023 [<font color="blue"><b>Spotlight</b></font>].<br>
    </li>

    <li>
        Rui Yang, <b>Yong Lin</b>, Xiaoteng Ma, Hao Hu, Chongjie Zhang, Tong Zhang. <br>
        <a href="https://arxiv.org/abs/2305.18882" target=&ldquo;blank&rdquo;>What Is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?</a> <br>
        ICML 2023 
    </li>

    <li>
        <b>Yong Lin*</b>, Renjie Pi*, Weizhong Zhang, Xiaobo Xia, Jiahui Gao, Xiao Zhou, Tongliang Liu, Bo Han. <br>
        <a href="https://openreview.net/forum?id=aFzaXRImWE" target=&ldquo;blank&rdquo;>A Holistic View of Noise Transition Matrix in Deep Learning and Beyond?</a> <br>
        ICLR 2023 [<font color="blue"><b>Spotlight</b></font>].
    </li>

    <li>
        <b>Yong Lin</b>, Shengyu Zhu, Lu Tan, Peng Cui. <br>
        <a href="https://openreview.net/forum?id=pUPFRSxfACD" target=&ldquo;blank&rdquo;>ZIN: When and How to Learn Invariance by Environment Inference?</a> <br>
        NeurIPS 2022 [<font color="blue"><b>Spotlight</b></font>].
    </li>
    

    <li>
        <b>Yong Lin*</b>, Hanze Dong*, Hao Wang, Tong Zhang. <br>
        <a href="https://proceedings.mlr.press/v162/zhou22d/zhou22d.pdf" target=&ldquo;blank&rdquo;>Bayesian Invariant Risk Minimization</a> <br>
        CVPR 2022 [<font color="blue"><b>Oral</b></font>].
    </li>

    <li>
        Xiao Zhou*, <b>Yong Lin*</b>, Weizhong Zhang*, Tong Zhang. <br>
        <a href="https://proceedings.mlr.press/v162/zhou22e/zhou22e.pdf" target=&ldquo;blank&rdquo;>Sparse Invariant Risk Minimization.</a> <br>
        ICML 2022.
    </li>

    <li>
        Xiao Zhou*, <b>Yong Lin*</b>, Renjie Pi*, Weizhong Zhang, Renzhe Xu, Peng Cui, Tong Zhang. <br>
        <a href="https://proceedings.mlr.press/v162/zhou22d/zhou22d.pdf" target=&ldquo;blank&rdquo;>Model Agnostic Sample Reweighting for Out-of-Distribution Learning.</a> <br>
        ICML 2022.
    </li>

    <li>
        <b>Yong Lin*</b>, Qing Lian* and Tong Zhang. <br>
        <a href="http://www.gatsby.ucl.ac.uk/~balaji/udl2021/accepted-papers/UDL2021-paper-044.pdf" target=&ldquo;blank&rdquo;>An Empirical Study of Invariant Risk Minimization on Deep Models.</a> <br>
        ICML2021 workshop on UDL.
    </li>

    <li>
        <b>Yong Lin</b>, Zheng Xu. <br>
        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7063235" target=&ldquo;blank&rdquo;>Cable sheath loss reduction strategy research based on the coupled linemodel.</a> <br>
        IEEE Transactions On Power Delivery.
    </li>

</ul>


<h2>Selected Awards</h2>
<ul>
    <li>
         Outstanding Paper Award of NAACL 2024
    </li>
    <li>
        2023 Apple Scholars in AI/ML PhD fellowship (22 awardees all over the world).
    </li>
    
    <li>
        Outstanding Graduate of Zhejiang Province, 2013.
    </li>
</ul>




<h2>Experiences</h2>
<ul>
    <li>
        <b> Princeton University </b>, Postdoc Fellow, 2024.9-now.
    </li>
    
    <li>
        <b> The Hong Kong University of Science and Technology </b>, PhD Student, 2020 - 2024.
    </li>

    <li>
        <b> Alibaba</b>, Senior Machine Learning Engineer, 2017 - 2020.
    </li>

    <li>
       <b> Zhejiang University </b>, Bachelor and Master Student (Ranking 1/207), 2009 - 2016.
    </li>
    
</ul>


<div id="footer">
    <div id="footer-text"></div>
    &copy 2024 Yong Lin | Last Update: 2025.2
</div>
</br>


</body></html>
