

<meta name="description" content="Yong Lin's homepage">
<link rel="stylesheet" href="./files/jemdoc.css" type="text/css">
<title>Yong Lin's Homepage</title>


<body>

    <style>
        /*********************************
         The list of publication items
         *********************************/
    /* The list of items */
    .biblist { }
    
    /* The item */
    .biblist li { }
    
    /* You can define custom styles for plstyle field here. */
    
    
    /*************************************
     The box that contain BibTeX code
     *************************************/
    div.noshow { display: none; }
    div.bibtex {
      margin-right: 0%;
      margin-top: 1.2em;
      margin-bottom: 1em;
      border: 1px solid silver;
      padding: 0em 1em;
      background: #ffffee;
    }
    div.bibtex pre { font-size: 80%; overflow: auto;  width: 100%; padding: 1em 0em;}</style>
    <script type="text/javascript">
        <!--
        // Toggle Display of BibTeX
        function toggleBibtex(articleid) {
            var bib = document.getElementById('bib_'+articleid);
            if (bib) {
                if(bib.className.indexOf('bibtex') != -1) {
                    bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex';
                }
            } else {
                return;
            }
        }
    -->
        </script>

<div id="layout-content" style="margin-top:25px">

<table>
    <tbody>
        <tr>
            <td width="670">
                <div id="toptitle">                 
                    <h1><b>Yong Lin</b>&nbsp;
                        </div>

                <h3>Ph.D. Student</h3>  
                <p>
                    <a href="https://hkust.edu.hk/" target=&ldquo;blank&rdquo;>Department of Computer Science and Engineering</a> <br>
                    <a href="https://hkust.edu.hk/" target=&ldquo;blank&rdquo;>Hong Kong  University of Science and Technology</a> <br>
                    Hong Kong<br>

                </p>
                <p>
                    <a href="https://scholar.google.com/citations?user=M4g0ZvMAAAAJ&hl=en"><b>[Google Scholar]</b></a>
                    <a href="https://github.com/linyongver" target="_blank"><b>[Github]</b></a>  <br>
                    E-mail: ylindf [at] connect [dot] ust [dot] hk<br>
                    
                </p>
            </td>
            <td>
                <!-- <img src="./files/photo.png" border="0" width="360"> -->
            </td>
        </tr><tr>
    </tr></tbody>
</table>
<h2>Short Bio</h2>
<p>
    [<font color="blue"><b>Postdoc and PhD Journey</b></font>] I am an incoming PostDoc Fellow at Princeton, jointly hosted by <a href="https://sites.google.com/view/cjin/home" target="_blank">Chi Jin</a> and <a href="https://pli.princeton.edu/" target="_blank">Princeton Language Intelligence</a>. I did my PhD research in <a href="https://tongzhang-ml.org/" target="_blank">Tong Zhang</a>'s group at the Hong Kong University of Science and Technology (HKUST). My research is generously supported by <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2023" target="_blank">Apple AI/ML PhD fellowship</a> and <a href="https://cerg1.ugc.edu.hk/hkpfs/index.html" target="_blank">Hong Kong PhD fellowship</a>.
</p>

<p>
    [<font color="blue"><b>Research Interests</b></font>] My research focuses on the trustworthiness and applications of machine learning. Key areas of interest include:
    <ul>
        <li>
            (1) Out-of-Distribution Generalization, such as enabling an autonomous driving system trained on city roads to navigate country roads; ensuring AI diagnostic systems trained on data from one hospital can reliably predict patients from another hospital. 
        </li>
        <li>
            (2) Alignment of AI systems, particularly Large Language Models (LLMs), to prioritize traits like helpfulness, harmlessness, and honesty. This entails ensuring that LLMs not only align with human preferences but also minimize the occurrence of false or misleading information, commonly known as "hallucinations". Recently, I have developed a strong interest in Reinforcement Learning with Human Preferences (RLHF), which is a promising way to achieve this goal.
        </li>
    </ul>
    
</p>

<p>
    [<font color="blue"><b>Pre PhD Journey</b></font>] Before starting my PhD studies, I served as a Senior Machine Learning Engineer at Alibaba, a prominent company in China similar to Amazon. During my tenure, I experienced firsthand the impressive capabilities of machine learning and developed industrial-level applications. Concurrently, I gained insights into the inherent challenges and instability of deep models in industrial settings.
</p>


<h2>Selected Papers</h2>
<p> (* denotes equal contribution.) </p>
<ul>
    
    <h3>Pre-prints</h3>
    
    <li>
        <b>Yong Lin*</b>, Hangyu Lin*, Wei Xiong*, Shizhe Diao*,[+8 authors], Han Zhao , Nan Jiang, Heng Ji, Yuan Yao, and Tong Zhang.<br>
        <a href="https://arxiv.org/pdf/2309.06256.pdf" target=&ldquo;blank&rdquo;>Mitigating the Alignment Tax of RLHF.</a> <br>              
        ACL ARR in submission. [<a href="https://github.com/avalonstrel/AdaptiveMA" target=&ldquo;blank&rdquo;> code </a>]  <br>
    </li>

    <li>
        <b>Yong Lin*</b>, Chen Liu*, Chenlu Ye*, Qing Lian, Yuan Yao, Tong Zhang. <br>
        <a href="https://arxiv.org/abs/2309.02476" target=&ldquo;blank&rdquo;>Optimal Sample Selection Through Uncertainty Estimation and Its Application in Deep Learning.</a> <br>              
        JMLR in submission. <br>
    </li>

    <li>
        Yifan Hao*, <b>Yong Lin*</b>, Difan Zou, Tong Zhang. <br>
        <a href="https://arxiv.org/pdf/2403.17592.pdf" target=&ldquo;blank&rdquo;>On the Benefits of Over-parameterization for Out-of-Distribution Generalization.</a> <br>              
        Pre-prints.<br>
    </li>
    


    <li>
        Qizhou Wang*, <b>Yong Lin*</b>, Yongqiang Chen*, Ludwig Schmidt, Bo Han, Tong Zhang<br>
        <a href="https://counteranimal.github.io/" target=&ldquo;blank&rdquo;>Do CLIPs Always Generalize Better than ImageNet Models?</a> <br>              
        ICLM2024 in submission.<br>
    </li>
    




    
    <h3>Publications</h3>

    
    <li>
        Haoxiang Wang*, <b>Yong Lin*</b>, Wei Xiong*, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang<br>
        <a href="https://arxiv.org/pdf/2402.18571.pdf" target=&ldquo;blank&rdquo;>Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards.</a> <br>              
        ACL 2024.<br>
    </li>

    <li>
        Hanning Zhang*, Shizhe Diao*, <b>Yong Lin*</b>, Yi R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, Tong Zhang. <br>
        <a href="https://arxiv.org/pdf/2311.09677.pdf" target=&ldquo;blank&rdquo;>R-tuning: Teaching large language models to refuse unknown questions.</a> <br>              
        NAACL 2024.<br>
    </li>
    
    <li>
        <b>Yong Lin*</b>, Lu Tan*, Yifan Hao*, Honam Wong, Hanze Dong, Weizhong Zhang, Yujiu Yang, Tong Zhang. <br>
        <a href="https://arxiv.org/pdf/2309.17230.pdf" target=&ldquo;blank&rdquo;>Spurious Feature Diversification Improves Out-of-distribution Generalization.</a> <br>   
        ICLR 2024.<br>
    </li>

    <li>
        Damien Teney, <b>Yong Lin</b>, Seong Joon Oh, Ehsan Abbasnejad. <br>
        <a href="https://arxiv.org/pdf/2209.00613" target=&ldquo;blank&rdquo;>Id and ood performance are sometimes inversely correlated on real-world datasets.</a> <br>   
        NeurIPS 2023 [<font color="blue"><b>Spotlight</b></font>].<br>
    </li>

    <li>
        Rui Yang, <b>Yong Lin</b>, Xiaoteng Ma, Hao Hu, Chongjie Zhang, Tong Zhang. <br>
        <a href="https://arxiv.org/abs/2305.18882" target=&ldquo;blank&rdquo;>What Is Essential for Unseen Goal Generalization of Offline Goal-conditioned RL?</a> <br>
        ICML 2023 
    </li>

    <li>
        <b>Yong Lin*</b>, Renjie Pi*, Weizhong Zhang, Xiaobo Xia, Jiahui Gao, Xiao Zhou, Tongliang Liu, Bo Han. <br>
        <a href="https://openreview.net/forum?id=aFzaXRImWE" target=&ldquo;blank&rdquo;>A Holistic View of Noise Transition Matrix in Deep Learning and Beyond?</a> <br>
        ICLR 2023 [<font color="blue"><b>Spotlight</b></font>].
    </li>

    <li>
        <b>Yong Lin</b>, Shengyu Zhu, Lu Tan, Peng Cui. <br>
        <a href="https://openreview.net/forum?id=pUPFRSxfACD" target=&ldquo;blank&rdquo;>ZIN: When and How to Learn Invariance by Environment Inference?</a> <br>
        NeurIPS 2022 [<font color="blue"><b>Spotlight</b></font>].
    </li>
    

    <li>
        <b>Yong Lin*</b>, Hanze Dong*, Hao Wang, Tong Zhang. <br>
        <a href="https://proceedings.mlr.press/v162/zhou22d/zhou22d.pdf" target=&ldquo;blank&rdquo;>Bayesian Invariant Risk Minimization</a> <br>
        CVPR 2022 [<font color="blue"><b>Oral</b></font>].
    </li>

    <li>
        Xiao Zhou*, <b>Yong Lin*</b>, Weizhong Zhang*, Tong Zhang. <br>
        <a href="https://proceedings.mlr.press/v162/zhou22e/zhou22e.pdf" target=&ldquo;blank&rdquo;>Sparse Invariant Risk Minimization.</a> <br>
        ICML 2022.
    </li>

    <li>
        Xiao Zhou*, <b>Yong Lin*</b>, Renjie Pi*, Weizhong Zhang, Renzhe Xu, Peng Cui, Tong Zhang. <br>
        <a href="https://proceedings.mlr.press/v162/zhou22d/zhou22d.pdf" target=&ldquo;blank&rdquo;>Model Agnostic Sample Reweighting for Out-of-Distribution Learning.</a> <br>
        ICML 2022.
    </li>

    <li>
        <b>Yong Lin*</b>, Qing Lian* and Tong Zhang. <br>
        <a href="http://www.gatsby.ucl.ac.uk/~balaji/udl2021/accepted-papers/UDL2021-paper-044.pdf" target=&ldquo;blank&rdquo;>An Empirical Study of Invariant Risk Minimization on Deep Models.</a> <br>
        ICML2021 workshop on UDL.
    </li>

    <li>
        <b>Yong Lin</b>, Zheng Xu. <br>
        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7063235" target=&ldquo;blank&rdquo;>Cable sheath loss reduction strategy research based on the coupled linemodel.</a> <br>
        IEEE Transactions On Power Delivery.
    </li>

</ul>


<h2>Selected Awards</h2>
<ul>
    <li>
        2023 Apple Scholars in AI/ML PhD fellowship (22 awardees all over the world).
    </li>
    <li>
        Hong Kong PhD Fellowship.
    </li>
    <li>
        National Scholarship  * 3 (1.8%, by China's Ministry of Education), 2010, 2011 and 2015.
    </li>
    
    <li>
        Outstanding Graduate of Zhejiang Province, 2013.
    </li>
</ul>




<h2>Experiences</h2>
<ul>
    <li>
        <b> The Hong Kong University of Science and Technology </b>, PhD Student, 2020 - Now.
    </li>

    <li>
        <b> Alibaba </b>, Senior Machine Learning Engineer, 2016 - 2020.
    </li>

    <li>
       <b> Zhejiang University </b>, Bachelor and Master Student (Ranking 1/207), 2009 - 2016.
    </li>
    
</ul>


<div id="footer">
    <div id="footer-text"></div>
    &copy 2024 Yong Lin | Last Update: 2024.2
</div>
</br>


</body></html>
